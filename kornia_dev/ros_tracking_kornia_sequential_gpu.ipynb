{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source /opt/ros/noetic/setup.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rosbag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkornia\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mK\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkornia\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mKF\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrosbag\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rosbag'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#import rospy\n",
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import rosbag\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sensor_msgs.msg import Image, JointState\n",
    "from message_filters import ApproximateTimeSynchronizer, Subscriber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "script_path = os.path.dirname(os.path.abspath(__file__))\n",
    "module_path = os.path.dirname(script_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "print(module_path)\n",
    "\n",
    "from RobotLink_kornia import *\n",
    "from StereoCamera_kornia import *\n",
    "from ParticleFilter_kornia import *\n",
    "from probability_functions_kornia import *\n",
    "from utils_kornia import *\n",
    "\n",
    "# File inputs\n",
    "robot_file    = script_path + '/../../fei_dataset/LND.json'\n",
    "camera_file   = script_path + '/../../fei_dataset/camera_calibration.yaml'\n",
    "hand_eye_file = script_path + '/../../fei_dataset/handeye.yaml'\n",
    "#robot_file    = script_path + '/../../journal_dataset/LND.json'\n",
    "#camera_file   = script_path + '/../../journal_dataset/camera_calibration.yaml'\n",
    "#hand_eye_file = script_path + '/../../journal_dataset/handeye.yaml'\n",
    "\n",
    "# ROS Topics\n",
    "left_camera_topic  = '/stereo/left/image'\n",
    "right_camera_topic = '/stereo/right/image'\n",
    "robot_joint_topic  = '/dvrk/PSM2/state_joint_current'\n",
    "robot_gripper_topic = '/dvrk/PSM2/state_jaw_current'\n",
    "#robot_joint_topic  = '/dvrk/PSM1/state_joint_current'\n",
    "#robot_gripper_topic = '/dvrk/PSM1/state_jaw_current'\n",
    "\n",
    "# main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Initalize ROS stuff here\n",
    "    #rospy.init_node('robot_tool_tracking', anonymous=True)\n",
    "    \n",
    "    # reference image directory\n",
    "    source_dir = 'kornia_dev/fei_ref_data/'\n",
    "    #source_dir = 'kornia_dev/ref_data/no_contour/'\n",
    "    draw_contours = False\n",
    "\n",
    "    # annotate output with detected lines\n",
    "    draw_lines = True\n",
    "\n",
    "    # crop parameters\n",
    "    in_file = source_dir + 'crop_scale.npy'\n",
    "    crop_scale = np.load(in_file)\n",
    "    print('crop_scale: {}'.format(crop_scale))\n",
    " \n",
    "    # ref line indices\n",
    "    in_file = source_dir + 'crop_ref_lines_l_idx.npy'\n",
    "    crop_ref_lines_l_idx = np.load(in_file) # torch.Size([2, 2, 2]) # endpoints per line: [y, x] [y, x]\n",
    "    \n",
    "    in_file = source_dir + 'crop_ref_lines_r_idx.npy'\n",
    "    crop_ref_lines_r_idx = np.load(in_file) # torch.Size([2, 2, 2]) # endpoints per line: [y, x] [y, x]\n",
    "\n",
    "    # ref lines\n",
    "    in_file = source_dir + 'crop_ref_lines_l.npy'\n",
    "    crop_ref_lines_l = np.load(in_file)\n",
    "    crop_ref_lines_l = torch.tensor(crop_ref_lines_l)\n",
    "\n",
    "    in_file = source_dir + 'crop_ref_lines_r.npy'\n",
    "    crop_ref_lines_r = np.load(in_file)\n",
    "    crop_ref_lines_r = torch.tensor(crop_ref_lines_r)\n",
    "    \n",
    "    # line descriptors\n",
    "    in_file = source_dir + 'crop_ref_desc_l.npy'\n",
    "    crop_ref_desc_l = np.load(in_file)\n",
    "    crop_ref_desc_l = torch.tensor(crop_ref_desc_l)\n",
    "\n",
    "    in_file = source_dir + 'crop_ref_desc_r.npy'\n",
    "    crop_ref_desc_r = np.load(in_file)\n",
    "    crop_ref_desc_r = torch.tensor(crop_ref_desc_r)\n",
    "    \n",
    "    # reference images\n",
    "    # left camera\n",
    "    crop_ref_l_img = source_dir + 'ref_left_img.jpg'\n",
    "    crop_ref_l_img = cv2.imread(crop_ref_l_img, cv2.IMREAD_COLOR)\n",
    "    crop_ref_l_img = cv2.cvtColor(crop_ref_l_img, cv2.COLOR_BGR2RGB)\n",
    "    img_dims = (int(crop_ref_l_img.shape[1]), int(crop_ref_l_img.shape[0]))\n",
    "    crop_ref_l_tensor = K.image_to_tensor(crop_ref_l_img).float() / 255.0 # [0, 1] torch.Size([3, 720, 1080]) torch.float32\n",
    "    crop_ref_l_tensor = K.enhance.sharpness(crop_ref_l_tensor, 5.0)\n",
    "    crop_ref_l_tensor = K.enhance.adjust_saturation(crop_ref_l_tensor, 5.0)\n",
    "    crop_ref_l_tensor = K.color.rgb_to_grayscale(crop_ref_l_tensor) # [0, 1] torch.Size([1, 720, 1080]) torch.float32\n",
    "\n",
    "    # right camera\n",
    "    crop_ref_r_img = source_dir + 'ref_right_img.jpg'\n",
    "    crop_ref_r_img = cv2.imread(crop_ref_r_img, cv2.IMREAD_COLOR)\n",
    "    crop_ref_r_img = cv2.cvtColor(crop_ref_r_img, cv2.COLOR_BGR2RGB)\n",
    "    crop_ref_r_tensor = K.image_to_tensor(crop_ref_r_img).float() / 255.0 # [0, 1] torch.Size([3, 720, 1080]) torch.float32\n",
    "    crop_ref_r_tensor = K.enhance.sharpness(crop_ref_r_tensor, 5.0)\n",
    "    crop_ref_r_tensor = K.enhance.adjust_saturation(crop_ref_r_tensor, 5.0)\n",
    "    crop_ref_r_tensor = K.color.rgb_to_grayscale(crop_ref_r_tensor) # [0, 1] torch.Size([1, 720, 1080]) torch.float32\n",
    "\n",
    "    # Load kornia model\n",
    "    model = KF.SOLD2(pretrained=True, config=None)\n",
    "\n",
    "    # parameters for shaft detection\n",
    "    canny_params = {\n",
    "        'use_canny': False,\n",
    "        'hough_rho_accumulator': 5.0,\n",
    "        'hough_theta_accumulator': 0.09,\n",
    "        'hough_vote_threshold': 100,\n",
    "        'rho_cluster_distance': 5.0,\n",
    "        'theta_cluster_distance': 0.09\n",
    "    }\n",
    "\n",
    "    kornia_params = {\n",
    "        'use_kornia': True,\n",
    "        'endpoints_to_polar': False,\n",
    "        'use_endpoint_intensities_only': True,\n",
    "        'endpoint_intensities_to_polar': False,\n",
    "        'search_radius': 10.0,\n",
    "        'intensity_params': {\n",
    "            'use_metric': 'pct',\n",
    "            'mean': 0,\n",
    "            'std': 1.0,\n",
    "            'pct': 10.0\n",
    "        },\n",
    "        'ransac_params': {\n",
    "            'num_iterations': 5,\n",
    "            'min_samples': 3.0,\n",
    "            'residual_threshold': 0.75,\n",
    "            'max_trials': 100,\n",
    "            'img_dims': img_dims\n",
    "        },\n",
    "        'use_line_intensities_only': False,\n",
    "        'line_intensities_to_polar': False\n",
    "    } \n",
    "\n",
    "    # video recording\n",
    "    record_video = True\n",
    "    fps = 30\n",
    "    if (record_video):\n",
    "\n",
    "        #out_file = source_dir + 'canny_left_video.mp4'\n",
    "        #out_file = source_dir + 'endp2p_left_video.mp4'\n",
    "        #out_file = source_dir + 'endpi_left_video.mp4'\n",
    "        #out_file = source_dir + 'endpi2p_left_video.mp4'\n",
    "        #out_file = source_dir + 'li_left_video.mp4'\n",
    "        #out_file = source_dir + 'li2p_left_video.mp4'\n",
    "        #left_video_out  = cv2.VideoWriter(out_file,  cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps, img_dims)\n",
    "\n",
    "        #out_file = source_dir + 'canny_right_video.mp4'\n",
    "        #out_file = source_dir + 'endp2p_right_video.mp4'\n",
    "        out_file = source_dir + 'endpi_right_video.mp4'\n",
    "        #out_file = source_dir + 'endpi2p_right_video.mp4'\n",
    "        #out_file = source_dir + 'li_right_video.mp4'\n",
    "        #out_file = source_dir + 'li2p_right_video.mp4'\n",
    "        right_video_out = cv2.VideoWriter(out_file, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), fps, img_dims)\n",
    "\n",
    "    # evaluation recording\n",
    "    #accuracy_file = None\n",
    "    #accuracy_file = open('kornia_dev/fei_ref_data/canny_accuracy.txt', 'w')\n",
    "    #accuracy_file = open('kornia_dev/fei_ref_data/endpoints_to_polar_accuracy.txt', 'w')\n",
    "    accuracy_file = open('kornia_dev/fei_ref_data/endpoint_intensities_only_accuracy.txt', 'w')\n",
    "    #accuracy_file = open('kornia_dev/fei_ref_data/endpoint_intensities_to_polar_accuracy.txt', 'w')\n",
    "    #accuracy_file = open('kornia_dev/fei_ref_data/line_intensities_only_accuracy.txt', 'w')\n",
    "    #accuracy_file = open('kornia_dev/fei_ref_data/line_intensities_to_polar_accuracy.txt', 'w')\n",
    "\n",
    "    #accuracy_file = open('kornia_dev/ref_data/no_contour/endpoint_intensities_only_accuracy.txt', 'w')\n",
    "\n",
    "    #localization_file = None\n",
    "    #localization_file = open('kornia_dev/fei_ref_data/canny_localization.txt', 'w')\n",
    "    #localization_file = open('kornia_dev/fei_ref_data/endpoints_to_polar_localization.txt', 'w')\n",
    "    localization_file = open('kornia_dev/fei_ref_data/endpoint_intensities_only_localization.txt', 'w')\n",
    "    #localization_file = open('kornia_dev/fei_ref_data/endpoint_intensities_to_polar_localization.txt', 'w')\n",
    "    #localization_file = open('kornia_dev/fei_ref_data/line_intensities_only_localization.txt', 'w')\n",
    "    #localization_file = open('kornia_dev/fei_ref_data/line_intensities_to_polar_localization.txt', 'w')\n",
    "\n",
    "    #localization_file = open('kornia_dev/ref_data/no_contour/endpoint_intensities_only_localization.txt', 'w')\n",
    "\n",
    "    robot_arm = RobotLink(robot_file, use_dh_offset=False) # position / orientation in Meters\n",
    "    cam = StereoCamera(camera_file, rectify = True, crop_scale = crop_scale, downscale_factor = 2, scale_baseline=1e-3)\n",
    "\n",
    "    # Load hand-eye transform \n",
    "    # originally in M\n",
    "    f = open(hand_eye_file)\n",
    "    hand_eye_data = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    cam_T_b = np.eye(4)\n",
    "    cam_T_b[:-1, -1] = np.array(hand_eye_data['PSM2_tvec'])/1000.0 # convert to mm\n",
    "    cam_T_b[:-1, :-1] = axisAngleToRotationMatrix(hand_eye_data['PSM2_rvec'])\n",
    "    #cam_T_b[:-1, -1] = np.array(hand_eye_data['PSM1_tvec'])/1000.0 # convert to mm\n",
    "    #cam_T_b[:-1, :-1] = axisAngleToRotationMatrix(hand_eye_data['PSM1_rvec'])\n",
    "\n",
    "    # Initialize filter\n",
    "    pf = ParticleFilter(num_states=6, # originally 9 (6 for lumped error + 3 for endowrist pitch/yaw/squeeze) -> 6 for just lumped error\n",
    "                        initialDistributionFunc=sampleNormalDistribution,\n",
    "                        motionModelFunc=additiveGaussianNoise,\n",
    "                        #motionModelFunc=lumpedErrorMotionModel,\n",
    "                        #obsModelFunc=pointFeatureObs,\n",
    "                        obsModelFunc=[\n",
    "                                    pointFeatureObs, \n",
    "                                    shaftFeatureObs_kornia\n",
    "                                    ],\n",
    "                        num_particles=200)\n",
    "\n",
    "\n",
    "    init_kwargs = {\n",
    "                    \"std\": np.array([1.0e-3, 1.0e-3, 1.0e-3, # pos # in M i.e. 1x10^-3 M\n",
    "                                    1.0e-2, 1.0e-2, 1.0e-2, # ori\n",
    "                                    #5.0e-3, 5.0e-3, 0.02\n",
    "                                    #0.0, 0.0, 0.0\n",
    "                                    ])   # joints\n",
    "                  }\n",
    "\n",
    "    pf.initializeFilter(**init_kwargs)\n",
    "    \n",
    "    #rospy.loginfo(\"Initialized particle filter\")\n",
    "       \n",
    "    # Main loop:\n",
    "    #rate = rospy.Rate(30) # 30hz\n",
    "    prev_joint_angles = None\n",
    "\n",
    "    bag = rosbag.Bag('../fei_dataset/volume_4points_t2.bag')\n",
    "    #bag = rosbag.Bag('../journal_dataset/stationary_camera_2020-06-24-15-49-10.bag')\n",
    "\n",
    "    old_l_img_msg = None\n",
    "    old_r_img_msg = None\n",
    "    old_j_msg = None\n",
    "    old_g_msg = None\n",
    "    l_img_msg = None\n",
    "    r_img_msg = None\n",
    "    j_msg = None\n",
    "    g_msg = None\n",
    "\n",
    "    msg_counter = 1\n",
    "\n",
    "    for topic, msg, t in bag.read_messages(topics=[left_camera_topic, right_camera_topic, robot_joint_topic, robot_gripper_topic]):\n",
    "\n",
    "        if topic == '/stereo/left/image':\n",
    "            old_l_img_msg = copy.deepcopy(l_img_msg)\n",
    "            l_img_msg = copy.deepcopy(msg)\n",
    "        if topic == '/stereo/right/image':\n",
    "            old_r_img_msg = copy.deepcopy(r_img_msg)\n",
    "            r_img_msg = copy.deepcopy(msg)\n",
    "        if topic == '/dvrk/PSM2/state_joint_current':\n",
    "            j_msg = copy.deepcopy(msg)\n",
    "        if topic == '/dvrk/PSM2/state_jaw_current':\n",
    "            g_msg = copy.deepcopy(msg)\n",
    "        #if topic == '/dvrk/PSM1/state_joint_current':\n",
    "            #j_msg = copy.deepcopy(msg)\n",
    "        #if topic == '/dvrk/PSM1/state_jaw_current':\n",
    "            #g_msg = copy.deepcopy(msg)\n",
    "        \n",
    "        try: \n",
    "            if ((l_img_msg != None) and (r_img_msg != None)) and ((l_img_msg != old_l_img_msg) or (r_img_msg != old_r_img_msg)) and (j_msg) and (g_msg):\n",
    "                _cb_left_img  = np.ndarray(shape=(l_img_msg.height, l_img_msg.width, 3), dtype=np.uint8, buffer=l_img_msg.data)\n",
    "                _cb_right_img = np.ndarray(shape=(r_img_msg.height, r_img_msg.width, 3), dtype=np.uint8, buffer=r_img_msg.data)\n",
    "                cb_joint_angles = np.array(j_msg.position + g_msg.position)\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        #if (msg_counter < 8338):\n",
    "            #msg_counter += 1\n",
    "            #continue\n",
    "        start_t = time.time()\n",
    "\n",
    "        # copy l/r images so not overwritten by callback\n",
    "        new_left_img = _cb_left_img.copy()\n",
    "        new_right_img = _cb_right_img.copy()\n",
    "\n",
    "        # process callback images\n",
    "        new_left_img, new_right_img = cam.processImage(new_left_img, new_right_img, crop_scale = crop_scale)\n",
    "        non_annotated_left_img = new_left_img.copy()\n",
    "        non_annotated_right_img = new_right_img.copy()\n",
    "\n",
    "        detected_keypoints_l, annotated_left_img  = segmentColorAndGetKeyPoints(non_annotated_left_img,  draw_contours = draw_contours)\n",
    "        new_detected_keypoints_l = np.copy(detected_keypoints_l)\n",
    "        detected_keypoints_r, annotated_right_img = segmentColorAndGetKeyPoints(non_annotated_right_img, draw_contours = draw_contours)\n",
    "        new_detected_keypoints_r = np.copy(detected_keypoints_r)\n",
    "\n",
    "        output_l  = detectShaftLines(\n",
    "                                    non_annotated_img = non_annotated_left_img,\n",
    "                                    annotated_img = annotated_left_img,\n",
    "                                    ref_img = crop_ref_l_img,\n",
    "                                    ref_tensor = crop_ref_l_tensor,\n",
    "                                    crop_ref_lines = crop_ref_lines_l,\n",
    "                                    crop_ref_lines_idx = crop_ref_lines_l_idx,\n",
    "                                    crop_ref_desc = crop_ref_desc_l,\n",
    "                                    model = model,\n",
    "                                    draw_lines = draw_lines,\n",
    "                                    canny_params = canny_params,\n",
    "                                    kornia_params = kornia_params\n",
    "                                    )\n",
    "        output_r  = detectShaftLines(\n",
    "                                    non_annotated_img = non_annotated_right_img,\n",
    "                                    annotated_img = annotated_right_img,\n",
    "                                    ref_img = crop_ref_r_img,\n",
    "                                    ref_tensor = crop_ref_r_tensor,\n",
    "                                    crop_ref_lines = crop_ref_lines_r,\n",
    "                                    crop_ref_lines_idx = crop_ref_lines_r_idx,\n",
    "                                    crop_ref_desc = crop_ref_desc_r,\n",
    "                                    model = model,\n",
    "                                    draw_lines = draw_lines,\n",
    "                                    canny_params = canny_params,\n",
    "                                    kornia_params = kornia_params\n",
    "                                    )\n",
    "\n",
    "\n",
    "        # copy new images to avoid overwriting by callback\n",
    "        new_left_img  = np.copy(output_l['new_img']) # cropped img w/detected lines\n",
    "        new_left_ref_img = np.copy(output_l['ref_img']) # cropped img w/ref line segments\n",
    "        new_right_img = np.copy(output_r['new_img']) # cropped img w/detected lines\n",
    "        new_right_ref_img = np.copy(output_r['ref_img']) # cropped img w/ref line segments\n",
    "        \n",
    "        # Nx2 array [[rho, theta], [rho, theta], ...]\n",
    "        new_canny_lines_l = np.copy(output_l['canny_lines']) \n",
    "        new_detected_endpoint_lines_l = np.copy(output_l['polar_lines_detected_endpoints']) # Nx2 array [[rho, theta], [rho, theta], ...]\n",
    "        new_endpoint_clouds_l =  np.copy(output_l['intensity_endpoint_clouds'])\n",
    "        new_endpoint_cloud_lines_l = np.copy(output_l['intensity_endpoint_lines'])\n",
    "        new_line_clouds_l = np.copy(output_l['intensity_line_clouds'])\n",
    "        new_line_cloud_lines_l = np.copy(output_l['intensity_line_lines'])\n",
    "\n",
    "        new_canny_lines_r = np.copy(output_r['canny_lines']) \n",
    "        new_detected_endpoint_lines_r = np.copy(output_r['polar_lines_detected_endpoints']) # Nx2 array [[rho, theta], [rho, theta], ...]\n",
    "        new_endpoint_clouds_r =  np.copy(output_r['intensity_endpoint_clouds'])\n",
    "        new_endpoint_cloud_lines_r = np.copy(output_r['intensity_endpoint_lines'])\n",
    "        new_line_clouds_r = np.copy(output_r['intensity_line_clouds'])\n",
    "        new_line_cloud_lines_r = np.copy(output_r['intensity_line_lines'])\n",
    "        \n",
    "        # copy new joint angles to ensure no overwrite\n",
    "        new_joint_angles = np.copy(cb_joint_angles)\n",
    "        \n",
    "        # update callback flag\n",
    "        new_cb_data = False\n",
    "\n",
    "        # First time\n",
    "        if prev_joint_angles is None:\n",
    "            prev_joint_angles = new_joint_angles\n",
    "        \n",
    "        # Predict Particle Filter\n",
    "        robot_arm.updateJointAngles(new_joint_angles)\n",
    "        #j_change = new_joint_angles - prev_joint_angles\n",
    "\n",
    "        #std_j = np.abs(j_change)*0.01\n",
    "        #std_j[-3:] = 0.0\n",
    "\n",
    "        # pred_kwargs = {\n",
    "        #                 \"std_pos\": 2.5e-5, # in Meters\n",
    "        #                 \"std_ori\": 1.0e-4,\n",
    "        #                 \"robot_arm\": robot_arm, \n",
    "        #                 \"std_j\": std_j,\n",
    "        #                 \"nb\": 4\n",
    "        #               }\n",
    "\n",
    "        pred_kwargs = {\n",
    "                        \"std\": [2.5e-5, 2.5e-5, 2.5e-5,  # in Meters THESE ARE THE MAIN TUNING PARAMETERS!\n",
    "                                1.0e-4, 1.0e-4, 1.0e-4] # in radians\n",
    "                    }\n",
    "        \n",
    "        pf.predictionStep(**pred_kwargs)\n",
    "        \n",
    "        # Update Particle Filter\n",
    "        upd_args = [  \n",
    "                    # pointFeatureObs arguments\n",
    "                    {\n",
    "                        'point_detections': (new_detected_keypoints_l, new_detected_keypoints_r),\n",
    "                        'robot_arm': robot_arm, \n",
    "                        'cam': cam, \n",
    "                        'cam_T_b': cam_T_b,\n",
    "                        'joint_angle_readings': new_joint_angles,\n",
    "                        'gamma': 0.5, # THIS IS A MAIN TUNING PARAMETER FOR FILTER PERFORMANCE https://github.com/ucsdarclab/dvrk_particle_filter/blob/master/config/ex_vivo_dataset_configure_filter.json\n",
    "                    },\n",
    "                    \n",
    "                    #shaftFeatureObs_kornia arguments\n",
    "                    {\n",
    "                        'use_lines': False,\n",
    "                        'use_clouds': 'endpoint_clouds',\n",
    "                        'detected_lines': {\n",
    "                            'canny': (new_canny_lines_l, new_canny_lines_r),\n",
    "                            'detected_endpoint_lines': (new_detected_endpoint_lines_l, new_detected_endpoint_lines_r),\n",
    "                            'endpoint_cloud_lines': (new_endpoint_cloud_lines_l, new_endpoint_cloud_lines_r),\n",
    "                            'line_cloud_lines': (new_line_cloud_lines_l, new_line_cloud_lines_r)\n",
    "                        },\n",
    "                        'intensity_clouds': {\n",
    "                            'endpoint_clouds': (new_endpoint_clouds_l, new_endpoint_clouds_r),\n",
    "                            'line_clouds': (new_line_clouds_l, new_line_clouds_r)\n",
    "                        },\n",
    "                        'robot_arm': robot_arm, \n",
    "                        'cam': cam, \n",
    "                        'cam_T_b': cam_T_b,\n",
    "                        'joint_angle_readings': new_joint_angles,\n",
    "                        'cost_assoc_params': {\n",
    "                            'gamma_rho': 0.05,  # THIS IS A MAIN TUNING PARAMETER FOR FILTER PERFORMANCE https://github.com/ucsdarclab/dvrk_particle_filter/blob/master/config/ex_vivo_dataset_configure_filter.json\n",
    "                            'gamma_theta': 7.5, # THIS IS A MAIN TUNING PARAMETER FOR FILTER PERFORMANCE https://github.com/ucsdarclab/dvrk_particle_filter/blob/master/config/ex_vivo_dataset_configure_filter.json\n",
    "                            'rho_thresh': 75,\n",
    "                            'theta_thresh': 0.5\n",
    "                        },\n",
    "                        'pixel_probability_params': {\n",
    "                            'sigma2_x': 5.0, #0.5\n",
    "                            'sigma2_y': 5.0, #0.5\n",
    "                        }\n",
    "                    }\n",
    "        ]\n",
    "\n",
    "        pf.updateStep(upd_args)\n",
    "        prev_joint_angles = new_joint_angles\n",
    "\n",
    "        correction_estimation = pf.getMeanParticle()\n",
    "\n",
    "        #rospy.loginfo(\"Time to predict & update {}\".format(time.time() - start_t))\n",
    "\n",
    "        # Project and draw skeleton\n",
    "        T = poseToMatrix(correction_estimation[:6])  \n",
    "        if correction_estimation.shape[0] > 6:\n",
    "            new_joint_angles[-(correction_estimation.shape[0]-6):] += correction_estimation[6:]\n",
    "        robot_arm.updateJointAngles(new_joint_angles)\n",
    "        img_list = projectSkeleton(robot_arm.getSkeletonPoints(), np.dot(cam_T_b, T), [new_left_img, new_right_img], cam.projectPoints, (new_detected_keypoints_l, new_detected_keypoints_r), accuracy_file, t, msg_counter)\n",
    "        img_list = drawShaftLines(robot_arm.getShaftFeatures(), cam, np.dot(cam_T_b, T), img_list)\n",
    "        #print('ros_tracking_kornia_sequential.py np.dot(cTb, T): {}'.format(np.dot(cam_T_b, T)))\n",
    "        #cv2.imshow(\"Left Img\",  img_list[0])\n",
    "        #cv2.imshow(\"Right Img\", img_list[1])\n",
    "\n",
    "        # video recording\n",
    "        if (record_video):\n",
    "            #print('img_list[0].shape: {}'.format(img_list[0].shape))\n",
    "            #print('type(img_list[0]): {}'.format(type(img_list[0])))\n",
    "            #left_video_out.write(img_list[0])\n",
    "            right_video_out.write(img_list[1])\n",
    "\n",
    "        text_string = str(t) + ',' + str(msg_counter) + ',' + str(new_joint_angles) + ',' + str(np.dot(cam_T_b, T)) + '\\n'\n",
    "        print(text_string)\n",
    "        if (localization_file):\n",
    "            localization_file.write(text_string)\n",
    "        \n",
    "        msg_counter += 1\n",
    "        print('msg_counter: {}'.format(msg_counter))\n",
    "        #cv2.waitKey(1)\n",
    "\n",
    "    if (accuracy_file):\n",
    "        accuracy_file.close()\n",
    "    if (localization_file):\n",
    "        localization_file.close()\n",
    "    print('end of bag, closing bag')\n",
    "    bag.close()\n",
    "    print('total number of messages: {}'.format(msg_counter))\n",
    "    print('Releasing video capture')\n",
    "    if (record_video):\n",
    "        #left_video_out.release()\n",
    "        right_video_out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
